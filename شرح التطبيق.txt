يمكن 


عندما تضع احد الاسئلة:

what are some good food recommendations in Los Angeles?

أنا أحب الأكل الشعبي والمشي

please recommend me a tour with good food and nice scenery?


يتم جلب المعلومات (Data): من الجهاز المحلي 
القرار والاختيار (Local Model): من الموديل المدرب محلياً (في حالة البحث عن جولات).
ثم يرسل الاجابة الى نمؤذج OpenAI لصياغة الرد 



نموذج @[backend/models/logistic_model_outputs.pkl] هو المخصص لمشروع، وفائدته تكمن في تحويل التطبيق من مجرد "شات بوت" عادي إلى "نظام تصنيف ذكي".

بدلاً من أن يبحث المستخدم عن اسم جولة محددة، يمكنه كتابة صفات يحبها (مثل: "أحب الجبال، والطقس الهادئ). هذا الموديل المدرب محلياً يقوم بتحويل هذه الكلمات إلى مصفوفة بيانات (0 و 1) ثم يتنبأ فوراً بأي من جولاتك السياحية هي الأنسب لهذا الشخص.
هذا النموذج لا يحتاج للإنترنت ليعرف أن "المشي + التصوير = جولة هوليوود". هو يقوم بهذا الربط داخلياً على الجهاز بسرعة فائقة (Microseconds)، مما يوفر استهلاك الـ Tokens من OpenAI ويجعل النظام "خبيراً" في بيانات الخاص بالجهاز فقط.

فمثلا عندما يسال المستخدم التطبيق يسأل الموديل logistic_model_outputs.pkl: "بناءً على السمات التي ذكرها المستخدم، ما هو المنتج (Product) الذي يجب أن أرشحه له؟ بمعني يتوقع الامكان لهذا المنتج 
الموديل يرد باسم المنتج (مثلاً: Wine Country Tour).
بعدها فقط، يتدخل OpenAI ليشرح للمستخدم لماذا هذا المنتج جيد.
نموذج Logistic Regression الملحق في هذا المشروع فهو "متخصص" فقط في ملف الجولات، ولن يرشح أبداً شيئاً خارج قائمتك.

لو حذفنا هذا الملف، سيصبح تطبيقك مجرد محرك بحث يبحث عن كلمات متشابهة. بوجود هذا الملف، يصبح تطبيقك برنامجاً ذكياً يتنبأ بما يريده المستخدم بناءً على خبرة سابقة (البيانات التي تدرب عليها)

يمكن تجربة كتابة اهتماماتك (مثل: I love history and walking) في التطبيق لترى كيف سيقوم هذا الموديل بترشيح جولة معينة لك؟



streamlit run streamlit-advanced-ai-app.py


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7260a3b2",
   "metadata": {},
   "source": [
    "# Python for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf1fa",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Generative AI and Agentic AI**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup our Python environment and see how we can run a series of Streamlit dashboards within our Google Colab workspace.\n",
    "\n",
    "![example-python-rag-app](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/app-python-rag.png)\n",
    "\n",
    "### Basic Application Flow\n",
    "\n",
    "We will be implementing 3 different versions of our Streamlit application using the following design with pure Python, `LangChain` and `LangGraph`\n",
    "\n",
    "![design-basic-app](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/design-basic-app.png)\n",
    "\n",
    "### Advanced Application Flow\n",
    "\n",
    "Finally we'll extend our application with more advanced components building on top of the `LangGraph` example to create something which more closely mimics what we might expect from a chat app like ChatGPT!\n",
    "\n",
    "![design-advanced-app](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/design-advanced-app.png)\n",
    "\n",
    "### Source Code\n",
    "\n",
    "* [Entire GitHub Repo](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/tree/main)\n",
    "* [Hello World Streamlit App](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-hello-world.py)\n",
    "* [Pure Python AI RAG App](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-python-rag.py)\n",
    "* [LangChain AI RAG App](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-langchain-rag.py)\n",
    "* [LangGraph AI RAG App](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-langgraph-rag.py)\n",
    "* [Advanced LangGraph AI RAG + ML Models App](https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-advanced-ai-app.py)\n",
    "\n",
    "**‚ö†Ô∏è BEFORE YOU BEGIN!**\n",
    "\n",
    "Make sure you‚Äôre using the **GPU runtime** in Google Colab for better performance when running local language models like TinyLlama.\n",
    "\n",
    "To enable GPU runtime, please go to the menu:  \n",
    "**Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**\n",
    "\n",
    "### Executing Code Cells\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or hit `command/shift + enter` when you're navigating to the cell.\n",
    "\n",
    "### Setup and Installation\n",
    "\n",
    "In the following cell we'll run a few commands to install the required Python packages and also grab all of our Streamlit application code and data artefacts from our GitHub repository.\n",
    "\n",
    "The cell below should around ~2 minutes to run as we'll need to download and cache a few large-language models from Hugging Face!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47247eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================\n",
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install --quiet faiss-cpu==1.11.0\n",
    "!pip install --quiet ctransformers==0.2.27\n",
    "!pip install --quiet dotenv==0.9.9\n",
    "!pip install --quiet pyngrok==7.2.12\n",
    "!pip install --quiet streamlit==1.47.1\n",
    "!pip install --quiet langchain_openai==0.3.28\n",
    "!pip install --quiet openai==1.98.0\n",
    "!pip install --quiet langchain==0.3.27\n",
    "!pip install --quiet langchain-community==0.3.27\n",
    "!pip install --quiet langchain-huggingface==0.3.1\n",
    "!pip install --quiet langgraph==0.6.2\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259.git data\n",
    "\n",
    "# Need to change directory into \"data\" to download git lfs data objects\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct\n",
    "%cd ..\n",
    "\n",
    "# Turn off future warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Frontload the necessary LLM and AI libraries we'll be using in this tutorial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "import os\n",
    "\n",
    "cache_root = \"data/models\"\n",
    "\n",
    "# this cache_dir is important as we'll reuse this in some of our Streamlit apps\n",
    "tiny_llama_model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    filename=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "    cache_dir=cache_root\n",
    ")\n",
    "\n",
    "# Download and cache our embedding model\n",
    "embedding_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "cached_embedding_model_path = snapshot_download(\n",
    "  repo_id=embedding_model_id,\n",
    "  cache_dir=cache_root\n",
    ")\n",
    "\n",
    "# We'll use this path to refer to our cached embedding model in our Streamlit apps\n",
    "print(cached_embedding_model_path)\n",
    "print(\"‚úÖ Initial setup steps complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e3f9a",
   "metadata": {},
   "source": [
    "# 1. Introduction to Streamlit\n",
    "\n",
    "Welcome to this hands-on tutorial! In this notebook, we‚Äôll be using Streamlit, a lightweight Python framework for building interactive web dashboards and AI apps ‚Äî all with minimal code.\n",
    "\n",
    "## 1.1 What is Streamlit?\n",
    "\n",
    "Streamlit lets you turn any Python script into a shareable web app using simple commands like st.write(), st.text_input(), and st.button().\n",
    "\n",
    "It's perfect for:\n",
    "\n",
    "- Visualizing data and model predictions\n",
    "- Prototyping dashboards\n",
    "- Building AI-powered tools quickly\n",
    "\n",
    "## 1.2 The Challenge in Google Colab\n",
    "\n",
    "Because Colab runs on remote virtual machines, we can‚Äôt directly access localhost ports like we would on our own computers.\n",
    "\n",
    "That means even if we run:\n",
    "\n",
    "```python\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "We still won't be able to visit localhost:8501 in our browser to view our application!\n",
    "\n",
    "## 1.3 Ngrok for Secure Tunnels\n",
    "\n",
    "To solve this, we‚Äôll use Ngrok, a tool that creates a secure tunnel from the internet to your Colab machine.\n",
    "\n",
    "We‚Äôll use the following pattern for all of our Streamlit apps in this tutorial:\n",
    "\n",
    "- Start running our Streamlit apps on port `8051` (we‚Äôll avoid `8501` to reduce potential conflicts)\n",
    "- Use Ngrok to expose port 8051 to a public web URL\n",
    "- Visit the generated public URL to view our live dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff40b3",
   "metadata": {},
   "source": [
    "# 2. Pre-flight Checks\n",
    "\n",
    "We will need to setup a few secure access components before we can proceed with our Streamlit and AI agents tutorial.\n",
    "\n",
    "## 2.1 Setup Ngrok Account\n",
    "\n",
    "1. Create Ngrok account at https://dashboard.ngrok.com/signup\n",
    "2. Acquire your `AuthToken` at https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "![ngrok-copy-auth-token](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/ngrok-copy-auth-token.png)\n",
    "\n",
    "## 2.2 Setup OpenRouter Account\n",
    "\n",
    "1. Create OpenRouter account at https://openrouter.ai/sign-up\n",
    "2. Create an API token at https://openrouter.ai/settings/keys with **read only** access and spend limit set to $0\n",
    "\n",
    "![open-router-create-api-key](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/open-router-create-api-key.png)\n",
    "\n",
    "3. Store this API key safely as you won't be able to see it again once you click away\n",
    "\n",
    "> **WARNING üö®üö®üö®** don't be like me and share your key publicly like in this image! This example is just for illustration and I've already deleted this set of API keys on my account!!!!!!! ü•µ\n",
    "\n",
    "![open-router-copy-api-key](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/open-router-copy-api-key.png)\n",
    "\n",
    "## 2.3 Streamlit Password\n",
    "\n",
    "We'll also secure our Streamlit apps by using a simple authentication password. This ensures that our publicly available URL still has a layer of authentication in-memory so only we can access it.\n",
    "\n",
    "Simply decide on a simple password for your Streamlit applications - but note that you'll need to type this when we start viewing our dashboards, so I wouldn't make it too long or complicated for a better learning experience!\n",
    "\n",
    "## 2.4 Setup Authentication\n",
    "\n",
    "Run the code cell immediately below to populate a local `.env` file with your sensitive information we setup in the previous step!\n",
    "\n",
    "Make sure you have the following ready so you can paste it in when prompted in the following cell:\n",
    "\n",
    "* Ngrok Auth Token\n",
    "* OpenRouter API Key\n",
    "* Streamlit password (remember to keep it short as we'll need to type it for every Streamlit app we run!)\n",
    "\n",
    "We‚Äôll ask for these values securely using getpass, and store them in a local `.env` file so they‚Äôre not exposed in the notebook.\n",
    "\n",
    "> **Warning üö®üö®üö®** Make sure to never share your Auth and API keys and be careful to avoid exposing them to the public especially when commiting files into GitHub! I've also setup a secure cleaup step to make sure we remove the `.env` file at [bottom of this notebook!](#final-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb81637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Secure .env Setup Script\n",
    "# =========================\n",
    "import getpass\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# Step 1: Check if .env file exists\n",
    "if os.path.exists(\".env\"):\n",
    "    print(\"‚úÖ .env file already exists!\")\n",
    "\n",
    "    # Load existing environment variables\n",
    "    dotenv.load_dotenv()\n",
    "\n",
    "    # Check each required variable individually\n",
    "    ngrok_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
    "    if not ngrok_token:\n",
    "        ngrok_token = getpass.getpass(\"üîê Paste your NGROK_AUTH_TOKEN: \")\n",
    "        with open(\".env\", \"a\") as f:\n",
    "            f.write(f'NGROK_AUTH_TOKEN={ngrok_token}\\n')\n",
    "\n",
    "    openrouter_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "    if not openrouter_key:\n",
    "        openrouter_key = getpass.getpass(\"üîê Paste your OPEN_ROUTER_API_KEY: \")\n",
    "        with open(\".env\", \"a\") as f:\n",
    "            f.write(f'OPEN_ROUTER_API_KEY={openrouter_key}\\n')\n",
    "\n",
    "    streamlit_password = os.getenv(\"STREAMLIT_PASSWORD\")\n",
    "    if not streamlit_password:\n",
    "        password_input = getpass.getpass(\"üîê Set your STREAMLIT_PASSWORD (press Enter to use default): \")\n",
    "        streamlit_password = password_input or \"linkedin-learning\"\n",
    "        if password_input == \"\":\n",
    "            print(f\"‚ÑπÔ∏è No password entered ‚Äî using default: {streamlit_password}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Streamlit password set!\")\n",
    "        with open(\".env\", \"a\") as f:\n",
    "            f.write(f'STREAMLIT_PASSWORD={streamlit_password}\\n')\n",
    "\n",
    "    print(\"‚úÖ All required environment variables are now set!\")\n",
    "\n",
    "# Step 2: If .env does not exist, prompt for everything\n",
    "else:\n",
    "    print(\"‚öôÔ∏è .env file not found ‚Äî let's create one now!\")\n",
    "\n",
    "    ngrok_token = getpass.getpass(\"üîê Paste your NGROK_AUTH_TOKEN: \")\n",
    "    openrouter_key = getpass.getpass(\"üîê Paste your OPEN_ROUTER_API_KEY: \")\n",
    "    password_input = getpass.getpass(\"üîê Set your STREAMLIT_PASSWORD (press Enter to use default): \")\n",
    "\n",
    "    streamlit_password = password_input or \"linkedin-learning\"\n",
    "    if password_input == \"\":\n",
    "        print(f\"‚ÑπÔ∏è No password entered ‚Äî using default: {streamlit_password}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Streamlit password set! (hidden)\")\n",
    "\n",
    "    with open(\".env\", \"w\") as f:\n",
    "        f.write(f'NGROK_AUTH_TOKEN={ngrok_token}\\n')\n",
    "        f.write(f'OPEN_ROUTER_API_KEY={openrouter_key}\\n')\n",
    "        f.write(f'STREAMLIT_PASSWORD={streamlit_password}\\n')\n",
    "\n",
    "    print(\"‚úÖ .env file created securely!\")\n",
    "\n",
    "# Step 3: Append model paths to .env\n",
    "with open(\".env\", \"a\") as f:\n",
    "    f.write(f'TINY_LLAMA_MODEL_PATH={tiny_llama_model_path}\\n')\n",
    "    f.write(f'EMBEDDING_MODEL_PATH={cached_embedding_model_path}\\n')\n",
    "print(\"‚úÖ Local model paths appended to .env file!\")\n",
    "\n",
    "# Finally - load in the newly created environment variables\n",
    "dotenv.load_dotenv()\n",
    "print(\"‚úÖ Latest .env file successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19c6d8",
   "metadata": {},
   "source": [
    "# 3. Streamlit Hello World App\n",
    "\n",
    "In the next section, we‚Äôll demonstrate how to run a basic Streamlit app to help verify everything is working smoothly before we dive deeper into our GenAI Streamlit applications.\n",
    "\n",
    "## 3.1 Streamlit Helper Functions\n",
    "\n",
    "We'll firstly define some Python helper functions to help us launch and shutdown our Streamlit applications using Ngrok to host our live dashboards.\n",
    "\n",
    "You can run the cell directly below to initialize these functions:\n",
    "\n",
    "* `launch_streamlit_in_colab`\n",
    "* `shutdown_streamlit_and_ngrok`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca350709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_streamlit_in_colab(app_path: str):\n",
    "    \"\"\"\n",
    "    Launch a Streamlit app within Google Colab using an ngrok tunnel.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    app_path : str\n",
    "        Path to the Streamlit `.py` app file to be launched (e.g., \"app.py\")\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    public_url : str\n",
    "        A publicly accessible ngrok URL that points to the Streamlit app running on port 8501.\n",
    "\n",
    "    This function performs:\n",
    "    ------------------------\n",
    "    - Authenticates with ngrok using a token stored in the .env file (`NGROK_AUTH_TOKEN`)\n",
    "    - Kills any existing ngrok tunnels\n",
    "    - Launches a new tunnel to port 8501\n",
    "    - Starts the Streamlit app in the background\n",
    "    - Waits briefly to allow the app to start up\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import time\n",
    "    from pyngrok import ngrok\n",
    "\n",
    "    # ----------------------------\n",
    "    # Set ngrok authentication token\n",
    "    # ----------------------------\n",
    "    ngrok.set_auth_token(os.getenv(\"NGROK_AUTH_TOKEN\"))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Kill any existing tunnels\n",
    "    # ----------------------------\n",
    "    ngrok.kill()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Open a new tunnel to port 8501\n",
    "    # ----------------------------\n",
    "    public_url = ngrok.connect(8501, \"http\")\n",
    "    print(f\"‚úÖ Streamlit app will be live at: {public_url}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Start the Streamlit app in the background\n",
    "    # ----------------------------\n",
    "    os.system(f\"streamlit run {app_path} &>/content/logs.txt &\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Wait a few seconds to ensure the app starts\n",
    "    # ----------------------------\n",
    "    time.sleep(3)\n",
    "\n",
    "    return public_url\n",
    "\n",
    "def shutdown_streamlit_and_ngrok():\n",
    "    \"\"\"\n",
    "    Shut down any running Streamlit app and terminate all ngrok tunnels in a Colab session.\n",
    "\n",
    "    This function performs:\n",
    "    ------------------------\n",
    "    - Kills all active ngrok tunnels using the pyngrok API\n",
    "    - Terminates any background Streamlit processes using `pkill`\n",
    "    - Provides feedback via console print statements\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    from pyngrok import ngrok\n",
    "\n",
    "    print(\"‚õî Shutting down Streamlit and ngrok...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Kill all active ngrok tunnels\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        ngrok.kill()\n",
    "        print(\"‚úÖ ngrok tunnel(s) killed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error killing ngrok: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Kill all background Streamlit processes\n",
    "    # ----------------------------\n",
    "    os.system(\"pkill -f streamlit\")\n",
    "    print(\"‚úÖ Streamlit processes killed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae70fae",
   "metadata": {},
   "source": [
    "## 3.2 Running our Streamlit Application\n",
    "\n",
    "Once we run the `launch_streamlit_in_colab(\"data/streamlit-hello-world.py\")` command in the following cell and you click on the shared URL link - we should see the following screens (if all things go well!)\n",
    "\n",
    "> ‚úÖ Make sure to click on the URL which looks like this: `\"https://<some-random-string>.ngrok-free.app\"` instead of `\"http://localhost:8501\"`\n",
    "\n",
    "After clicking on this URL - you should see this welcome page from Ngrok with your own unique random URL which should match what the one you clicked on from the Colab notebook. It's fine to click on `Visit Site` to proceed!\n",
    "\n",
    "![streamlit-hello-world-ngrok](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/streamlit-hello-world-ngrok.png)\n",
    "\n",
    "You should first be welcomed by an authentication page.\n",
    "\n",
    "![streamlit-hello-world-login](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/streamlit-hello-world-login.png)\n",
    "\n",
    "Then following this - you should see text input asking for a prompt.\n",
    "\n",
    "![streamlit-hello-world-prompt](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/streamlit-hello-world-prompt.png)\n",
    "\n",
    "Finally - if all goes well, you should see something like this output - try it out using your own prompts!\n",
    "\n",
    "![streamlit-hello-world-success](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/streamlit-hello-world-success.png)\n",
    "\n",
    "The source code for the Hello World app can be viewed within the Streamlit app directly - and also in the GitHub repo for this course:\n",
    "\n",
    "* https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/blob/main/streamlit-hello-world.py\n",
    "\n",
    "After you're done with this hello world example - you can run the cell directly below to stop running the Streamlit application and continue with our tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a300c",
   "metadata": {},
   "source": [
    "## 3.3 Launch Streamlit App\n",
    "\n",
    "Run the following cell to get started!\n",
    "\n",
    "> ‚úÖ Make sure to click on the URL which looks like this: `\"https://<some-random-string>.ngrok-free.app\"` instead of `\"http://localhost:8501\"`\n",
    "\n",
    "If anything goes wrong or you see an error about Ngrok only allowing one live tunnel at a time - simply run the `shutdown_streamlit_and_ngrok()` command in the following cell below and it everything should resolve itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Streamlit app in Colab\n",
    "launch_streamlit_in_colab(\"data/streamlit-hello-world.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70ad04",
   "metadata": {},
   "source": [
    "## 3.4 Shutdown Streamlit and Ngrok\n",
    "\n",
    "Once you're ready to move onto the next component - we can shutdown our Streamlit app and free up the Ngrok tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Streamlit app in Colab\n",
    "shutdown_streamlit_and_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2ce2c",
   "metadata": {},
   "source": [
    "# 4. Pure Python LLM App\n",
    "\n",
    "In the next cell - we'll run a Streamlit app which details a simple `Retrieval-Augmented-Generation` (RAG) pipeline using pure Python only.\n",
    "\n",
    "![design-basic-app](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/design-basic-app.png)\n",
    "\n",
    "This Streamlit app is a **Retrieval-Augmented Generation (RAG)** system that answers natural language travel queries about California.\n",
    "\n",
    "It combines:\n",
    "- Local semantic search using SentenceTransformer + FAISS  \n",
    "- Cloud-based reasoning with OpenRouter's Mistral LLM  \n",
    "- Structured data on locations and tour products\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **User submits a travel question**  \n",
    "   Example: ‚ÄúWhat are some scenic spots in Yosemite?‚Äù\n",
    "\n",
    "2. **Local Semantic Search (FAISS + Embeddings)**  \n",
    "   The app searches over a dataset of location descriptions using vector similarity.\n",
    "\n",
    "3. **Tour Product Matching**  \n",
    "   The app finds the most relevant tour products based on matched locations.\n",
    "\n",
    "4. **LLM Response via OpenRouter API**  \n",
    "   Retrieved context and products are sent to a hosted Mistral model to:\n",
    "   - Generate a structured answer\n",
    "   - Suggest 3 follow-up questions\n",
    "\n",
    "5. **Interactive Chat Interface**  \n",
    "   Users can click follow-up suggestions to continue the conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components\n",
    "\n",
    "| Component               | Purpose                                               |\n",
    "|-------------------------|--------------------------------------------------------|\n",
    "| `SentenceTransformer`   | Encodes queries and location/product text as embeddings |\n",
    "| `FAISS`                 | Finds the most similar documents by cosine similarity   |\n",
    "| `OpenRouter API`        | Sends prompts to a hosted Mistral LLM                  |\n",
    "| `Streamlit`             | Manages UI and chat interactions                       |\n",
    "| `products.csv`          | Tour product descriptions and metadata                 |\n",
    "| `locations.csv`         | Descriptions of California travel destinations         |\n",
    "\n",
    "---\n",
    "\n",
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "This app demonstrates a classic RAG pipeline:\n",
    "\n",
    "- **Retrieve**: Use FAISS to find top location matches  \n",
    "- **Augment**: Insert retrieved context into an LLM prompt  \n",
    "- **Generate**: Let the model answer the question and suggest follow-ups\n",
    "\n",
    "---\n",
    "\n",
    "## Login and Session Memory\n",
    "\n",
    "- Password protection using `.env` configuration  \n",
    "- `st.session_state` is used to remember:\n",
    "  - Chat history\n",
    "  - Suggested follow-ups\n",
    "  - Retrieved context and products\n",
    "\n",
    "---\n",
    "\n",
    "## Example Output Format\n",
    "\n",
    "```markdown\n",
    "**Answer:** Yosemite is known for granite cliffs and waterfalls...\n",
    "\n",
    "**Suggested Follow-Ups:**\n",
    "- What are the best hikes for beginners?\n",
    "- Can I do a day trip from San Francisco?\n",
    "- What wildlife can I see in Yosemite?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa225bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Streamlit app in Colab\n",
    "launch_streamlit_in_colab(\"data/streamlit-python-rag.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254eccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Streamlit app in Colab\n",
    "shutdown_streamlit_and_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791a1a",
   "metadata": {},
   "source": [
    "# 5. LangChain App\n",
    "\n",
    "In the next cell - we'll run a Streamlit app which extends our pure Python app by replacing certain components using `LangChain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LangChain app\n",
    "launch_streamlit_in_colab(\"data/streamlit-langchain-rag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057dbe8e",
   "metadata": {},
   "source": [
    "\n",
    "## App Comparisons\n",
    "\n",
    "This outlines the key differences between the original Python-only RAG travel assistant and the updated LangChain-based version. It focuses on what learners should look out for when reading the source code.\n",
    "\n",
    "---\n",
    "\n",
    "| Feature               | Python RAG App                                  | LangChain Version                                      |\n",
    "|------------------------|--------------------------------------------------|---------------------------------------------------------|\n",
    "| Vector Search          | Manual FAISS index + SentenceTransformer        | LangChain `FAISS` vectorstore abstraction              |\n",
    "| Embedding Model        | Loaded and applied manually                     | Handled via `HuggingFaceEmbeddings` object             |\n",
    "| Prompt Construction    | f-strings and manual formatting                 | `ChatPromptTemplate` with structured system/human roles|\n",
    "| LLM API Usage          | Direct OpenAI-compatible API via `openai`       | `ChatOpenAI` from LangChain                            |\n",
    "| Memory / History       | Manual tracking in `st.session_state`           | `ConversationBufferMemory` + `StreamlitChatMessageHistory` |\n",
    "| Chain Logic            | Custom function to call LLM                     | Encapsulated in `LLMChain`                             |\n",
    "| Output Parsing         | Manual split on markdown delimiters             | Still manual (same logic reused)                       |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Enhancements in the LangChain Version\n",
    "\n",
    "### 1. Prompt Management\n",
    "- Python app uses plain strings to define prompts.\n",
    "- LangChain uses `ChatPromptTemplate`, separating roles (`system`, `human`) and keeping the prompt modular and readable.\n",
    "\n",
    "### 2. LLM Calls\n",
    "- Python app constructs API requests manually.\n",
    "- LangChain wraps LLM usage into `LLMChain`, managing context, input variables, and memory under the hood.\n",
    "\n",
    "### 3. Memory Integration\n",
    "- Original app stores chat history manually using session state.\n",
    "- LangChain provides a structured memory system that integrates with the chain directly, simplifying history reuse.\n",
    "\n",
    "### 4. Vector Store\n",
    "- In the original, embeddings are encoded and searched with raw FAISS APIs.\n",
    "- In LangChain, documents are wrapped in `Document` objects and indexed with a single line:  \n",
    "  ```python\n",
    "  FAISS.from_documents(docs, embedder)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## Learner Takeaways\n",
    "\n",
    "| Topic                      | Why It Matters                                                |\n",
    "|----------------------------|---------------------------------------------------------------|\n",
    "| Prompt Engineering         | LangChain improves readability and makes prompts reusable.   |\n",
    "| Chat Memory                | LangChain handles turn history without extra logic.           |\n",
    "| Abstractions vs. Control   | Python app gives full control; LangChain offers clean structure. |\n",
    "| Chain Composition          | LangChain chains are modular and composable.                  |\n",
    "| Debugging and Transparency | Both approaches expose full prompt and output when needed.    |\n",
    "\n",
    "---\n",
    "\n",
    "## Code Differences to Inspect\n",
    "\n",
    "| Section                    | What to Compare                                               |\n",
    "|----------------------------|---------------------------------------------------------------|\n",
    "| `get_response_and_followups()` vs. `rag_chain.invoke()` | LLM invocation logic         |\n",
    "| Prompt formatting          | f-strings vs. `ChatPromptTemplate`                            |\n",
    "| Vector search logic        | Manual FAISS search vs. `similarity_search()`                |\n",
    "| Memory structure           | `st.session_state[\"chat_history\"]` vs. LangChain memory object |\n",
    "| Follow-up parsing          | Identical logic reused in both implementations               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Streamlit app in Colab\n",
    "shutdown_streamlit_and_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64001e",
   "metadata": {},
   "source": [
    "# 5. LangGraph Agentic AI\n",
    "\n",
    "In the next cell - we'll run a Streamlit app which further extends upon our static AI workflows using `LangChain` to an autonomous agentic AI framework using `LangGraph`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a056858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LangGraph app\n",
    "launch_streamlit_in_colab(\"data/streamlit-langgraph-rag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cde9b9",
   "metadata": {},
   "source": [
    "\n",
    "Now we can compare the three implementations of the Explore California AI Travel Assistant:\n",
    "\n",
    "- Python-only RAG app\n",
    "- LangChain-powered RAG app\n",
    "- LangGraph-powered agentic app\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Feature               | Python RAG App         | LangChain App              | LangGraph App                      |\n",
    "|------------------------|------------------------|-----------------------------|-------------------------------------|\n",
    "| Vector Search          | Manual FAISS           | LangChain FAISS wrapper     | LangGraph FAISS wrapper             |\n",
    "| Embeddings             | SentenceTransformer    | `HuggingFaceEmbeddings`     | `HuggingFaceEmbeddings`             |\n",
    "| Prompt Handling        | Manual strings         | `ChatPromptTemplate`        | LLM prompt built inside tool node   |\n",
    "| LLM Integration        | Raw OpenAI client       | `ChatOpenAI`                | `ChatOpenAI`                        |\n",
    "| Memory/History         | Custom session state   | `ConversationBufferMemory`  | Passed explicitly in state graph    |\n",
    "| Orchestration          | Manual logic           | `LLMChain`                  | `StateGraph` with conditional flow  |\n",
    "| Follow-up Handling     | Manual UI logic        | Button + LLM                | Embedded in graph output state      |\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes LangGraph Different?\n",
    "\n",
    "- **Graph-Based Execution**: Logic is encoded as a conditional flow graph using `StateGraph`, enabling dynamic control over the sequence of tools.\n",
    "- **Tool Functions**: Each tool (`@tool`) is a modular, typed function. Tools represent semantic retrieval, product matching, or LLM generation.\n",
    "- **State Management**: Entire app logic runs via a dictionary `AppState`, making flow reproducible and observable.\n",
    "- **Follow-Up Flow**: LangGraph can bypass location retrieval if the user is asking a follow-up‚Äîcontrolled by a dynamic node.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Flow Differences\n",
    "\n",
    "```\n",
    "Python RAG:            query -> embed -> FAISS -> LLM -> UI\n",
    "\n",
    "LangChain:             query -> FAISS -> LLMChain(memory+prompt) -> UI\n",
    "\n",
    "LangGraph:             query -> entry_selector\n",
    "                           ‚Ü≥ search_locations ‚Üí match_products ‚Üí generate_answer\n",
    "                           ‚Ü≥ generate_answer (if follow-up)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept                  | LangGraph Emphasis                          |\n",
    "|--------------------------|---------------------------------------------|\n",
    "| Modularity               | Breaks up functionality into reusable tools |\n",
    "| Graph-Oriented Thinking  | Encourages declarative, inspectable flows   |\n",
    "| Tool Inputs/Outputs      | Fully typed and validated                   |\n",
    "| Control Flow             | Uses `entry_selector` to branch logic       |\n",
    "| Production Readiness     | Easier to extend with observability/logging |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e31303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Streamlit app in Colab\n",
    "shutdown_streamlit_and_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f899b",
   "metadata": {},
   "source": [
    "# 6. Advanced AI Application\n",
    "\n",
    "In the next cell - we'll run a Streamlit app which further extends our `LangGraph` app to incorporate our logistic regression ML model and complex LLM based routing and summarization with the ability to save chat history and get personalized recommendations.\n",
    "\n",
    "We've also extended this application to generate token usage as part of the overall LLM metadata response outputs so we can track usage.\n",
    "\n",
    "![design-advanced-app](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/design-advanced-app.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run advanced Streamlit LangGraph app\n",
    "launch_streamlit_in_colab(\"data/streamlit-advanced-ai-app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b4033",
   "metadata": {},
   "source": [
    "\n",
    "## Basic and Advanced App Comparison\n",
    "\n",
    "The following compares two LangGraph-based versions of the Explore California AI Travel Assistant:\n",
    "\n",
    "- **LangGraph (Basic)**: Simple conditional RAG flow for answering questions\n",
    "- **LangGraph (Advanced)**: Enhanced multi-modal pipeline using attributes, predictions, and topic routing\n",
    "\n",
    "---\n",
    "\n",
    "| Feature                          | LangGraph Basic                        | LangGraph Advanced                                |\n",
    "|----------------------------------|----------------------------------------|---------------------------------------------------|\n",
    "| Retrieval                        | Locations only                         | Locations + semantic user attributes              |\n",
    "| ML Integration                   | None                                   | Uses ML model for product prediction              |\n",
    "| Topic Routing                    | Boolean flag (follow-up or not)        | LLM-generated decision (search vs. attributes)    |\n",
    "| Context Sources                  | Always from query                      | Can come from ML prediction + product metadata    |\n",
    "| Chat History Management          | Basic list                             | Per-chat sessions with metadata + topic label     |\n",
    "| UI Complexity                    | Basic chat + follow-up                 | Full saved chat manager with reloadable sessions  |\n",
    "| LLM Call                         | `llm.invoke()`                         | `llm.generate()` (with token usage tracking)      |\n",
    "| Attributes Vector Index          | Not included                           | Semantic search over defined user attribute space |\n",
    "| Extensibility                    | Medium                                 | High ‚Äì modular tools and dynamic routing          |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Enhancements in the Advanced App\n",
    "\n",
    "### 1. Attribute-Based Personalization\n",
    "- Queries like ‚ÄúI like scenic drives and local food‚Äù are routed to a tool that finds similar predefined attributes (using FAISS).\n",
    "- These attributes are passed to a classifier that predicts the most relevant travel product.\n",
    "\n",
    "### 2. Intelligent Routing\n",
    "- A single LLM call decides the user intent: attribute-based suggestion, factual question, or follow-up.\n",
    "- Also generates a topic label for session organization.\n",
    "\n",
    "### 3. Chat Session Management\n",
    "- Each conversation is stored with its context, product info, and topic.\n",
    "- Sidebar allows learners to reload past conversations or start new ones.\n",
    "\n",
    "### 4. Token Tracking\n",
    "- Uses `llm.generate()` to record token usage for analysis and cost-awareness.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Comparison\n",
    "\n",
    "```\n",
    "Basic:\n",
    "query ‚Üí entry_selector ‚Üí (search ‚Üí match ‚Üí generate) or (generate only)\n",
    "\n",
    "Advanced:\n",
    "query ‚Üí entry_selector\n",
    "  ‚îú‚îÄ‚îÄ‚Üí find_attributes ‚Üí predict_product ‚Üí get_product_locations ‚Üí generate\n",
    "  ‚îú‚îÄ‚îÄ‚Üí search_locations ‚Üí match_products ‚Üí generate\n",
    "  ‚îî‚îÄ‚îÄ‚Üí generate (follow-up)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept                        | Advanced App Highlights                         |\n",
    "|--------------------------------|--------------------------------------------------|\n",
    "| Modularity                     | Clear separation of tools (search, match, predict, generate) |\n",
    "| Agentic Workflow               | Tools form nodes in a decision graph            |\n",
    "| Personalization via ML         | Product recommendation based on binary features |\n",
    "| Graph Routing via LLM          | Conditional control flow without hardcoding     |\n",
    "| Reusability                    | Components can be swapped, extended, or reused  |\n",
    "| Observability                  | Result view shows full LLM inputs/outputs       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Streamlit app in Colab\n",
    "shutdown_streamlit_and_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041df54",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've explored multiple versions of the Explore California AI Travel App, here are some practical next steps to help you deepen your understanding and prepare your app for production use!\n",
    "\n",
    "### 1. Explore the Codebase\n",
    "- Each Streamlit app (`RAG`, `LangChain`, `LangGraph`, and `Advanced LangGraph`) is fully open-source.\n",
    "- Inspect how components are defined, including embedding models, FAISS indexes, and LangChain/LangGraph workflows.\n",
    "- Pay attention to how state is managed across user sessions and how tools are composed in LangGraph.\n",
    "\n",
    "### 2. Deploy to Streamlit Cloud\n",
    "- You can deploy any of the apps directly to Streamlit Community Cloud.\n",
    "- Make sure to include a `.streamlit/secrets.toml` file with your `OPEN_ROUTER_API_KEY` and any required credentials.\n",
    "- Optionally connect it to a GitHub repository to enable continuous updates.\n",
    "\n",
    "### 3. Add Python Unit Tests\n",
    "- Create unit tests for key components such as:\n",
    "  - Semantic search functions (e.g., `search_locations`)\n",
    "  - Product matching logic\n",
    "  - ML-based predictions\n",
    "  - LLM formatting and parsing functions\n",
    "- Use `pytest` or `unittest`, and mock external calls (e.g., LLMs or FAISS) for faster test runs.\n",
    "\n",
    "### 4. Refactor into Modular Components\n",
    "- Split large monolithic `app.py` files into:\n",
    "  - `models/` ‚Üí ML model loading and prediction\n",
    "  - `tools/` ‚Üí LangGraph tools and utility functions\n",
    "  - `components/` ‚Üí Streamlit UI components\n",
    "  - `graphs/` ‚Üí LangGraph workflows\n",
    "  - `services/` ‚Üí LLM API interaction and embedding logic\n",
    "- This makes the codebase easier to test, maintain, and extend.\n",
    "\n",
    "### 5. Extend the Application\n",
    "- Add multi-turn memory using LangChain agents or LangGraph memory nodes.\n",
    "- Add feedback logging or analytics to capture query usage and LLM performance.\n",
    "- Add multilingual support with translation layers or region-specific filters.\n",
    "- Extend ML product recommendations to use user history, clustering, or collaborative filtering.\n",
    "\n",
    "---\n",
    "\n",
    "By breaking the app into composable, testable, and deployable pieces, you're well on your way to building production-grade AI applications using RAG, LangChain, and LangGraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615fa2d",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "\n",
    "Run the following cell block to remove all your keys and sensitive information from the Google Colab instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d936e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - kill any ngrok tunnels and delete sensitive .env file\n",
    "import os\n",
    "import dotenv\n",
    "from pyngrok import ngrok\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    \n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "\n",
    "    # Check if NGROK_AUTH_TOKEN is set and stop any existing tunnels\n",
    "    if os.getenv(\"NGROK_AUTH_TOKEN\"):\n",
    "        ngrok.set_auth_token(os.getenv(\"NGROK_AUTH_TOKEN\"))\n",
    "\n",
    "        # Stop any existing Ngrok tunnels\n",
    "        ngrok.kill()\n",
    "        print(\"‚úÖ Ngrok tunnels stopped.\")\n",
    "    \n",
    "    # Remove the .env file\n",
    "    os.remove(\".env\")\n",
    "    print(\"‚úÖ .env file removed safely.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

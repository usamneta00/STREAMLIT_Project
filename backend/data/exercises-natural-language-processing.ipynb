{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7260a3b2",
   "metadata": {},
   "source": [
    "# Python for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf1fa",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup our Python environment and get started with our Explore California NLP exercises.\n",
    "\n",
    "**‚ö†Ô∏è BEFORE YOU BEGIN!**\n",
    "\n",
    "Make sure you‚Äôre using the **GPU runtime** in Google Colab for better performance when running local language models like TinyLlama.\n",
    "\n",
    "To enable GPU runtime, please go to the menu:  \n",
    "**Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e1c66",
   "metadata": {},
   "source": [
    "### Challenge Exercises\n",
    "\n",
    "1. Explore our `locations` NLP dataset\n",
    "2. Implement keyword, TF-IDF, BM-25 and semantic search functionality\n",
    "3. Setup simple Retrival-Augmented-Generation (RAG) AI workflow using a local Hugging Face LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd55533",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or hit `command/shift + enter` to run individual cells one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install --quiet rank_bm25==0.2.2\n",
    "!pip install --quiet faiss-cpu==1.11.0\n",
    "!pip install --quiet ctransformers==0.2.27\n",
    "!pip install --quiet dotenv==0.9.9\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259.git data\n",
    "\n",
    "# Need to change directory into \"data\" to download git lfs data objects\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct\n",
    "%cd ..\n",
    "\n",
    "# Turn off future warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load in Sentence Transformers first as it takes a little while\n",
    "# We use this for embedding text into a dense vector space\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llama_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    filename=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b60251",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "We'll begin by calculating the following using our `text_data` column from `locations_df` - this is the equivalent of the structured HTML data we'll extract from the webpage from the Explore California website.\n",
    "\n",
    "* Total count of locations\n",
    "* Vocabulary size and most frequent keywords\n",
    "* Generate a word cloud for `text_data` and compare this with the simplified `description` field\n",
    "* Create sentence embeddings and visualize clusters in 3D space to identify similar locations based off their `descriptions`\n",
    "\n",
    "Let's first load in our `locations` datasets using Pandas and we'll get started by exploring our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f552f",
   "metadata": {},
   "source": [
    "## 1.1 Load Locations Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c51b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load in locations dataset\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "# View first few rows of dataframe\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617dc26",
   "metadata": {},
   "source": [
    "## 1.2 Locations Analysis\n",
    "\n",
    "> How many unique locations are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique locations are there?\n",
    "print(f\"There are {len(locations_df)} unique locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cab075",
   "metadata": {},
   "source": [
    "> How many locations per category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many locations per category?\n",
    "locations_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f2217",
   "metadata": {},
   "source": [
    "> How many locations per region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many locations per region?\n",
    "locations_df[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8d84d",
   "metadata": {},
   "source": [
    "## 1.3 NLP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b76450",
   "metadata": {},
   "source": [
    "### 1.3.1 Most Frequent Terms\n",
    "\n",
    "> Identify the top 25 most frequent terms across all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Identify the Top 25 Most Frequent Terms Across All Locations\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Used for tokenizing and counting word frequencies\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Initialize CountVectorizer\n",
    "# ---------------------------------------\n",
    "# We use uni-gram tokenization (single words) and automatically remove common English stopwords\n",
    "# This helps us focus on meaningful content-specific terms\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Fit the vectorizer to the text data\n",
    "# ---------------------------------------\n",
    "# The input is a list of raw text strings from the 'text_data' column (assumed to be pre-cleaned)\n",
    "# This will tokenize each document and build a term-document matrix\n",
    "X = vectorizer.fit_transform(locations_df['text_data'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Aggregate total term frequencies across all documents\n",
    "# ---------------------------------------\n",
    "# Get the list of all terms (vocabulary)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum up the count of each term across all documents\n",
    "term_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Create a DataFrame showing each term and its total count\n",
    "word_freq = pd.DataFrame({\n",
    "    'term': terms,\n",
    "    'count': term_counts\n",
    "})\n",
    "\n",
    "# Sort terms by descending frequency\n",
    "word_freq = word_freq.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Output results\n",
    "# ---------------------------------------\n",
    "print(f\"Total number of unique uni-gram terms: {len(word_freq)}\")\n",
    "\n",
    "# Display the top 25 most frequent terms\n",
    "word_freq.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb194b2",
   "metadata": {},
   "source": [
    "### 1.3.2 Word Cloud Visualization\n",
    "\n",
    "We'll use our HTML `text_data` and the summary `description` data to build 2 word clouds and compare them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a644435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Goal: Compare Frequent Terms in 'text_data' vs 'description' Using Side-by-Side Word Clouds\n",
    "# ---------------------------------------\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Prepare text inputs\n",
    "# ---------------------------------------\n",
    "wordcloud_text_inputs = \" \".join(locations_df['text_data'])\n",
    "wordcloud_description_inputs = \" \".join(locations_df['description'])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Generate WordClouds\n",
    "# ---------------------------------------\n",
    "wordcloud_text = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_text_inputs)\n",
    "\n",
    "wordcloud_description = WordCloud(\n",
    "    width=800, height=400, background_color='white'\n",
    ").generate(wordcloud_description_inputs)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Plot the WordClouds side-by-side\n",
    "# ---------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))  # Create 1 row, 2 column layout\n",
    "\n",
    "# Left: WordCloud for HTML text_data\n",
    "axes[0].imshow(wordcloud_text, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Top Terms in HTML Data\", fontsize=24)\n",
    "\n",
    "# Right: WordCloud for description\n",
    "axes[1].imshow(wordcloud_description, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"Top Terms in Summary Description\", fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6105a0f",
   "metadata": {},
   "source": [
    "### 1.3.2 Embedding and Visualizing Location Descriptions\n",
    "\n",
    "In this exercise, we‚Äôre using a lightweight **Sentence Transformer** model (`all-MiniLM-L6-v2`) to turn each location‚Äôs `description` into an **embedding** ‚Äî a numeric representation of its meaning.\n",
    "\n",
    "These models are built using the **Sentence-BERT (SBERT)** architecture, which extends BERT to efficiently produce **sentence-level embeddings** that can be compared using cosine similarity. This allows us to capture semantic meaning ‚Äî not just exact word overlap ‚Äî in a compact vector format.\n",
    "\n",
    "We‚Äôll then explore the data using two unsupervised techniques:\n",
    "\n",
    "- **KMeans clustering** groups together descriptions that are semantically similar ‚Äî think of it as sorting locations by theme.\n",
    "- **t-SNE** helps us reduce the 384-dimensional embeddings down to just 3 dimensions so we can visualize them in a chart.\n",
    "\n",
    "Finally, we plot the results in a **3D interactive Plotly scatter plot**. Each point represents a location, and clusters help reveal patterns in how different descriptions relate to each other.\n",
    "\n",
    "üëâ **Tip:** You can click on cluster names in the legend to isolate them, and use your mouse or trackpad to **pan, zoom, and rotate** around the 3D space for a better view.\n",
    "\n",
    "![](https://raw.githubusercontent.com/LinkedInLearning/applied-AI-and-machine-learning-for-data-practitioners-5932259/main/images/embedding-workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 3D Visualization of Location Descriptions Using Sentence Embeddings, Clustering, and t-SNE\n",
    "# ---------------------------------------\n",
    "\n",
    "# Import core libraries\n",
    "from sentence_transformers import SentenceTransformer      # For embedding text into dense vector space\n",
    "from sklearn.cluster import KMeans                         # For clustering the text embeddings\n",
    "from sklearn.manifold import TSNE                          # For dimensionality reduction (to 3D)\n",
    "import plotly.express as px                                # For interactive plotting\n",
    "import plotly.io as pio                                    # For controlling plotly renderers in Colab\n",
    "\n",
    "# Ensure that Plotly renders correctly in Google Colab\n",
    "pio.renderers.default = 'colab'\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Encode Descriptions into Embeddings\n",
    "# ---------------------------------------\n",
    "# We use a pre-trained SentenceTransformer model to convert free-text location descriptions\n",
    "# into dense semantic vectors that capture meaning\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(locations_df['description'].tolist())\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Apply KMeans Clustering on the Embeddings\n",
    "# ---------------------------------------\n",
    "# We assign each location to one of 5 semantic clusters using unsupervised learning\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Reduce Embedding Dimensionality to 3D with t-SNE\n",
    "# ---------------------------------------\n",
    "# t-SNE projects high-dimensional embeddings into 3D space for visualization,\n",
    "# preserving local similarity structure\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "embedding_3d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Prepare Data for Plotting\n",
    "# ---------------------------------------\n",
    "# Select only the relevant columns from the original dataset\n",
    "locations_plotting_df = locations_df.loc[:, [\"location_name\", \"description\"]]\n",
    "\n",
    "# Create a shortened version of the description (first 25 characters) for concise tooltips\n",
    "locations_plotting_df[\"short_description\"] = locations_plotting_df['description'].str[:25] + '...'\n",
    "\n",
    "# Assign cluster labels as strings for categorical coloring in the legend\n",
    "locations_plotting_df['cluster'] = 'Cluster ' + clusters.astype(str)\n",
    "\n",
    "# Add the 3D coordinates from the t-SNE projection\n",
    "locations_plotting_df['x'] = embedding_3d[:, 0]\n",
    "locations_plotting_df['y'] = embedding_3d[:, 1]\n",
    "locations_plotting_df['z'] = embedding_3d[:, 2]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 5: Build an Interactive 3D Scatter Plot with Plotly\n",
    "# ---------------------------------------\n",
    "# Each point represents a location, colored by its cluster, and can be explored in 3D\n",
    "fig = px.scatter_3d(\n",
    "    locations_plotting_df,\n",
    "    x='x', y='y', z='z',                      # 3D coordinates\n",
    "    color='cluster',                         # Use cluster for color grouping\n",
    "    hover_data={                             # Tooltip configuration\n",
    "        'location_name': True,\n",
    "        'short_description': True,\n",
    "        'x': False, 'y': False, 'z': False\n",
    "    },\n",
    "    title=\"Embedding Location Descriptions\",\n",
    "    labels={'x': 't-SNE X', 'y': 't-SNE Y'},   # Axis labels for readability\n",
    "    # Order clusters explicitly in the legend\n",
    "    category_orders={'cluster': sorted(locations_plotting_df['cluster'].unique())}\n",
    "\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 6: Format Plot Appearance\n",
    "# ---------------------------------------\n",
    "# Make the chart larger and cleaner with defined width, height, and no axis margins\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Tune the size and transparency of the plot markers\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 7: Render the Interactive Chart\n",
    "# ---------------------------------------\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811ed7c",
   "metadata": {},
   "source": [
    "### 1.3.3 Inspecting Embeddings and Plotting Data\n",
    "\n",
    "To better understand what‚Äôs happening under the hood, it‚Äôs helpful to look at the actual data we‚Äôre working with:\n",
    "\n",
    "- **Raw embeddings:** After encoding the descriptions with `SentenceTransformer`, each location is represented as a 384-dimensional vector. These embeddings reflect the semantic meaning of each description and are the foundation for clustering and visualization.\n",
    "\n",
    "- **Plotting DataFrame:** The `locations_plotting_df` DataFrame brings everything together ‚Äî original fields like `location_name`, the `short_description`, assigned `cluster`, and the 3D coordinates from t-SNE. This structure helps connect the original text with the transformed features used for plotting.\n",
    "\n",
    "Exploring these structures can help you connect the Python code to the actual data transformations ‚Äî from text, to vectors, to clusters, to 3D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the embedding for Yosemite at index 0\n",
    "print(f\"Raw embedding values for Yosemite National Park (length = {len(embeddings[0])})\")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_plotting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c177b0",
   "metadata": {},
   "source": [
    "# 2. Implementing Search for Explore California\n",
    "\n",
    "In this section, we‚Äôll build out three different **search functionalities** for our case study: **Explore California**. These techniques mirror how a travel website might power its **search experience** ‚Äî helping users find tours and destinations based on what they type in.\n",
    "\n",
    "We'll explore and compare the following search algorithms:\n",
    "\n",
    "1. **Keyword Search**  \n",
    "   A simple approach that checks if the user‚Äôs query appears directly in the text.\n",
    "\n",
    "2. **TF-IDF (Term Frequency‚ÄìInverse Document Frequency)**  \n",
    "   A classic information retrieval technique that scores how important a term is in a document relative to the rest of the dataset.\n",
    "\n",
    "3. **BM25 (Best Matching 25)**  \n",
    "   An advanced, ranking-based algorithm that improves on TF-IDF by accounting for term frequency saturation and document length.\n",
    "\n",
    "Once we‚Äôve implemented and compared these traditional search methods, we‚Äôll move on to a more modern approach ‚Äî performing another round of **embeddings**, this time using a more advanced **Sentence-BERT** model to enable **semantic search** based on meaning rather than just words.\n",
    "\n",
    "This will set the stage for building more intelligent and flexible retrieval systems, similar to what powers modern AI-driven search experiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d9591",
   "metadata": {},
   "source": [
    "## 2.1 Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# üîé Unified Search Comparison for Explore California\n",
    "# ---------------------------------------\n",
    "# In this cell, we implement 4 search strategies:\n",
    "# 1. Keyword Search\n",
    "# 2. TF-IDF Vector Search\n",
    "# 3. BM25 Ranking\n",
    "# 4. Semantic Search using Sentence-BERT\n",
    "# Each approach is defined as a function so we can easily compare results for the same query.\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer         # For TF-IDF vector search\n",
    "from rank_bm25 import BM25Okapi                                     # For BM25 ranking\n",
    "from sentence_transformers import SentenceTransformer, util        # For semantic search with SBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 1: Prepare the Corpus for Search\n",
    "# ---------------------------------------\n",
    "# We'll search against our detailed HTML data in the text_data field\n",
    "corpus = locations_df[\"text_data\"].fillna('').tolist()\n",
    "\n",
    "# Tokenize for BM25 (required format: list of lists of words)\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 2: Precompute Representations for Search\n",
    "# ---------------------------------------\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# BM25 indexing\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Sentence-BERT embeddings for semantic similarity (this is the same as above!)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Define Reusable Search Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def search_keyword(query, top_k=5):\n",
    "    \"\"\"Return rows that contain the query term (case-insensitive substring match).\"\"\"\n",
    "    results = [i for i, doc in enumerate(corpus) if query.lower() in doc.lower()]\n",
    "    return locations_df.iloc[results][:top_k]\n",
    "\n",
    "def search_tfidf(query, top_k=5):\n",
    "    \"\"\"Rank documents by TF-IDF cosine similarity with the query.\"\"\"\n",
    "    q_vec = tfidf_vectorizer.transform([query])\n",
    "    scores = np.dot(tfidf_matrix, q_vec.T).toarray().ravel()\n",
    "    top_indices = scores.argsort()[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "def search_bm25(query, top_k=5):\n",
    "    \"\"\"Rank documents by BM25 relevance score.\"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "def search_semantic(query, top_k=5):\n",
    "    \"\"\"Rank documents by semantic similarity using Sentence-BERT embeddings.\"\"\"\n",
    "    query_emb = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy()\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return locations_df.iloc[top_indices]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 4: Define a Reusable Comparison Function for Search Outputs\n",
    "# ---------------------------------------\n",
    "# This function lets us easily compare the results of all four search methods\n",
    "# ‚Äî Keyword Match, TF-IDF, BM25, and Semantic SBERT ‚Äî\n",
    "# for any given query. It's useful for validating how different search techniques\n",
    "# interpret and rank the same input phrase.\n",
    "# ---------------------------------------\n",
    "\n",
    "def compare_search_methods(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Run and display results from all four search methods for a given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search string to evaluate\n",
    "    - top_k (int): Number of top results to return for each method (default = 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keyword Search (exact substring match)\n",
    "    print(f\"\\nüîç Keyword Search: '{query}'\")\n",
    "    display(search_keyword(query, top_k=top_k))\n",
    "\n",
    "    # TF-IDF Vector Search (weighted match based on term rarity)\n",
    "    print(f\"\\nüß† TF-IDF Search: '{query}'\")\n",
    "    display(search_tfidf(query, top_k=top_k))\n",
    "\n",
    "    # BM25 Ranking (term frequency-aware ranking algorithm)\n",
    "    print(f\"\\nüìö BM25 Search: '{query}'\")\n",
    "    display(search_bm25(query, top_k=top_k))\n",
    "\n",
    "    # Semantic Search using Sentence-BERT embeddings\n",
    "    print(f\"\\nü§ñ Semantic Search (SBERT): '{query}'\")\n",
    "    display(search_semantic(query, top_k=top_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d352b",
   "metadata": {},
   "source": [
    "## 2.2 Comparing Search Methods\n",
    "\n",
    "Now that we‚Äôve implemented all four search strategies ‚Äî **Keyword**, **TF-IDF**, **BM25**, and **Semantic Search (SBERT)** ‚Äî we‚Äôll use our `compare_search_methods` function to evaluate how each method handles different types of queries.\n",
    "\n",
    "We‚Äôll start with a few simple, direct queries like:\n",
    "\n",
    "> *‚Äúwine tours‚Äù*\n",
    "\n",
    "Then move on to more natural, conversational questions such as:\n",
    "\n",
    "> *‚ÄúWhere can I find a good place to workout?‚Äù*\n",
    "\n",
    "This comparison helps reveal the strengths and limitations of each approach:\n",
    "\n",
    "- **Keyword Search** is very strict ‚Äî it only returns results that contain the exact words from the query, so it often misses related or reworded content.\n",
    "- **TF-IDF** and **BM25** offer more flexibility by considering word importance and frequency, though they still rely on exact token overlap.\n",
    "- **Semantic Search (SBERT)** shines on open-ended or vague queries ‚Äî it can understand meaning even when the wording doesn‚Äôt match exactly, making it ideal for natural language search.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Try it yourself:**  \n",
    "Run the `compare_search_methods()` function with a few of your own queries to see how the results change.  \n",
    "You can also adjust the `top_k` parameter to control how many top matches you want to return (e.g., 3, 5, 10).\n",
    "\n",
    "This is a great way to explore how traditional vs. AI-powered search behaves in a realistic scenario!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be657df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"wine\",\n",
    "  top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21972678",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"wine tours\",\n",
    "  top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"nice beaches near SoCal\",  # SoCal is short for Southern California\n",
    "  top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search_methods(\n",
    "  query=\"Where can I find a good place to workout?\",\n",
    "  top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b9dbb",
   "metadata": {},
   "source": [
    "# 3. Retrieval Augmented Generation LLM Workflow\n",
    "\n",
    "In this next section, we‚Äôll use a **local LLM (TinyLlama)** to answer natural language questions about our Explore California dataset.\n",
    "\n",
    "We‚Äôll try two approaches:\n",
    "\n",
    "1. **Direct Q&A (No Context):**  \n",
    "   First, we‚Äôll ask the model a few questions without providing any background or external knowledge. This helps us see how well a small, locally-run model performs ‚Äúout of the box.‚Äù\n",
    "\n",
    "2. **Contextual Q&A using RAG:**  \n",
    "   Next, we‚Äôll implement a simple **retrieval-augmented generation (RAG)** pipeline using the **semantic embeddings** we created earlier. We‚Äôll:\n",
    "   - Use **`all-MiniLM-L6-v2`**, a Sentence-BERT model, to embed each location‚Äôs description into a semantic vector\n",
    "   - Build a **FAISS** index from those embeddings for fast similarity search\n",
    "   - Retrieve the most relevant descriptions for a given query\n",
    "   - Insert those retrieved descriptions into the LLM‚Äôs prompt as context\n",
    "   - Ask the same question again ‚Äî and compare how much better the answers become\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**How we‚Äôre running TinyLlama locally via Google Colab:**  \n",
    "We‚Äôll be using the **GGUF version of TinyLlama** provided by [TheBloke](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) on Hugging Face, which is optimized for local inference.  \n",
    "This model runs with the **`ctransformers`** library and supports fast, quantized loading with minimal memory ‚Äî perfect for running on Google Colab.\n",
    "\n",
    "You‚Äôll specify the quantized model file (e.g., `Q4_K_M`) and load it directly via `AutoModelForCausalLM.from_pretrained()`.\n",
    "\n",
    "---\n",
    "\n",
    "üîç **Why this matters:**  \n",
    "Local models like TinyLlama are fast and run offline, but they come with limitations ‚Äî especially around **context size** (the number of tokens they can read at once). This can limit how much background knowledge we can include in a single request.\n",
    "\n",
    "As we move forward, we‚Äôll also explore **cloud-based LLMs** via API calls (like OpenAI, Mistral, or Claude), which support much larger context windows and often produce more accurate and detailed responses ‚Äî especially for complex or multi-turn questions.\n",
    "\n",
    "This exercise will help you understand the tradeoffs between small local models and larger cloud-hosted ones ‚Äî and how retrieval can help bridge that gap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd004f93",
   "metadata": {},
   "source": [
    "## 3.1 Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# üß† Retrieval-Augmented Generation (RAG) using TinyLlama + MiniLM + FAISS\n",
    "# ---------------------------------------\n",
    "# This script compares two approaches to answering questions using a local LLM:\n",
    "# 1. Direct prompt to TinyLlama (no context)\n",
    "# 2. Retrieval-Augmented Generation (RAG) using semantic search + FAISS + TinyLlama\n",
    "#\n",
    "# Key Components:\n",
    "# - SentenceTransformer (MiniLM) for embeddings\n",
    "# - FAISS for fast similarity search\n",
    "# - TinyLlama (GGUF, via ctransformers) for local inference\n",
    "# - Token counting to check prompt size before sending to LLM\n",
    "# ---------------------------------------\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Import dependencies\n",
    "# ---------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import re\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Prepare your dataset\n",
    "# ---------------------------------------\n",
    "# Assumes locations_df has a 'description' column with clean, descriptive text\n",
    "corpus = locations_df[\"description\"].fillna(\"\").tolist()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Generate sentence embeddings using MiniLM\n",
    "# ---------------------------------------\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")                  # Lightweight and fast\n",
    "sbert_embeddings = sbert_model.encode(corpus, convert_to_numpy=True)  # Convert to NumPy for FAISS\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Build a FAISS index for fast similarity search\n",
    "# ---------------------------------------\n",
    "dimension = sbert_embeddings.shape[1]  # 384 for MiniLM\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(sbert_embeddings)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Load the TinyLlama local model\n",
    "# ---------------------------------------\n",
    "llama_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    filename=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")\n",
    "\n",
    "tiny_llama_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_path,\n",
    "    model_type=\"llama\",         # Required for TinyLlama GGUF format\n",
    "    gpu_layers=16,              # Start at 16 but need to make sure Colab supports GPU acceleration\n",
    "    max_new_tokens=512          # Increased output length for richer answers\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6. Utility: Estimate prompt token count\n",
    "# ---------------------------------------\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimate token count using basic word+punctuation splitting.\n",
    "    Approximate ‚Äî useful for checking against 512-token context window.\n",
    "    \"\"\"\n",
    "    return len(re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE))\n",
    "\n",
    "# ---------------------------------------\n",
    "# 7. Define QA response functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def generate_direct_response(query):\n",
    "    \"\"\"\n",
    "    Answer question using TinyLlama with no background knowledge.\n",
    "    Helps evaluate zero-shot performance on local models.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Answer the question as a helpful travel agent based only on your knowledge about California\n",
    "### Question:\\n{query}\\n\\n\n",
    "### Response:\n",
    "    \"\"\"\n",
    "    return tiny_llama_llm(prompt)\n",
    "\n",
    "def generate_rag_response(query, k=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Answer question using RAG: retrieve top-k similar descriptions using FAISS\n",
    "    and inject them as context for TinyLlama to use in its response.\n",
    "    Also prints estimated token count for visibility.\n",
    "    \"\"\"\n",
    "    # Semantic search using query embedding\n",
    "    query_embedding = sbert_model.encode(query, convert_to_numpy=True)\n",
    "    _, top_indices = index.search(query_embedding.reshape(1, -1), k)\n",
    "\n",
    "    # Build context: include both location name and description\n",
    "    context_rows = locations_df.iloc[top_indices[0]]\n",
    "    context = \"\\n\".join(\n",
    "        f\"{row['location_name']}: {row['description']}\" for _, row in context_rows.iterrows()\n",
    "    )\n",
    "\n",
    "    # Construct a formatted prompt with context and question\n",
    "    prompt = f\"\"\"Answer the question as a helpful travel agent using only the provided context.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    # Estimate tokens in the prompt\n",
    "    prompt_tokens = count_tokens(prompt)\n",
    "    if verbose:\n",
    "        print(f\"üìè Estimated tokens in prompt: {prompt_tokens}\")\n",
    "    \n",
    "    # Stop if token limit exceeded\n",
    "    if prompt_tokens > 512:\n",
    "        raise ValueError(\"üö´ Prompt exceeds TinyLlama's 512-token context limit. Try reducing `k` or trimming the context.\")\n",
    "\n",
    "    # Generate response using TinyLlama and return as dict along with context\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": tiny_llama_llm(prompt)\n",
    "    }\n",
    "\n",
    "# ---------------------------------------\n",
    "# 8. Compare Direct vs RAG-Enhanced Responses\n",
    "# ---------------------------------------\n",
    "\n",
    "# This is a convenience function to print long strings into multiple lines\n",
    "def wrap_print(text):\n",
    "    print(textwrap.fill(text, width=80))\n",
    "\n",
    "def compare_llm_responses(query, k=3):\n",
    "    \"\"\"\n",
    "    Compare TinyLlama's response to a query with and without RAG-enhanced context.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The natural language question to ask the model\n",
    "    - k (int): Number of top matching documents to retrieve for RAG\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nüß† Query: {query}\")\n",
    "    \n",
    "    # üîπ Step 1: Run Direct Q&A with no background context\n",
    "    print(\"\\nü§ñ Direct LLM Response (No Context Provided):\")\n",
    "    direct_response = generate_direct_response(query)\n",
    "    wrap_print(direct_response)\n",
    "    \n",
    "    # üîπ Step 2: Run RAG-enhanced Q&A\n",
    "    print(\"\\nüì° Running Retrieval-Augmented Generation (RAG)...\")\n",
    "    rag_response = generate_rag_response(query, k=k)\n",
    "\n",
    "    # üîπ Step 3: Show enhanced prompt that will be fed into the model\n",
    "    print(f\"\\nüìö Enhanced Prompt With Retrieved Context (Top {k} Documents):\")\n",
    "    print(rag_response[\"prompt\"])\n",
    "\n",
    "    # üîπ Step 4: Show the final model response with RAG\n",
    "    print(\"\\nüí¨ RAG-Enhanced Response (With Context):\")\n",
    "    wrap_print(rag_response[\"response\"])\n",
    "    print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bab894",
   "metadata": {},
   "source": [
    "### 3.2 Comparing Simple vs Conversational Queries\n",
    "\n",
    "Now that we‚Äôve set up both direct querying and our RAG (retrieval-augmented generation) pipeline, it‚Äôs time to test how well they perform across different types of natural language queries.\n",
    "\n",
    "We‚Äôll start with **simple, keyword-style queries** ‚Äî like `\"scenic hikes\"` or `\"wine tours\"` ‚Äî that resemble what a user might type into a traditional search box.\n",
    "\n",
    "Then we‚Äôll move on to more **conversational, open-ended questions**, such as:\n",
    "\n",
    "- `\"Where can I go kayaking or canoeing in California?\"`\n",
    "- `\"What are some good places for stargazing?\"`\n",
    "- `\"I'm traveling with kids ‚Äî any family-friendly adventures?\"`\n",
    "\n",
    "By comparing the answers returned by:\n",
    "\n",
    "- A **direct local LLM (TinyLlama)** without any context, and  \n",
    "- The same model enhanced with **retrieved context from FAISS**,  \n",
    "\n",
    "we‚Äôll get a better sense of **how retrieval helps smaller models** understand and respond more accurately ‚Äî especially as the queries become more natural and less keyword-focused.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Important Notes:**\n",
    "\n",
    "- **Responses may vary slightly each time you run them.**\n",
    "- **Token limits matter.** If the prompt (context + question) exceeds TinyLlama‚Äôs 512-token limit, it may raise an error or truncate output.\n",
    "- **üí° Recommendation:** Keep `top_k` at or below **5** to avoid exceeding the token limit.  \n",
    "  If you include too many documents in the prompt, the model may not be able to read the full context effectively.\n",
    "- **Try it yourself:**  \n",
    "  - Experiment by entering your own questions  \n",
    "  - Adjust the `top_k` parameter to include more or fewer context items  \n",
    "  - Re-run the same query to see how the results might vary across runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"scenic hikes\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2120c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"wine tours\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"Where can I go kayaking or canoeing in California?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d796586",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"What are some good places for stargazing?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_llm_responses(\"I'm traveling with kids ‚Äî any family-friendly adventures?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da289eb",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "## Local Model Limitations: Context Size and Compute Constraints\n",
    "\n",
    "While running **local LLMs** like TinyLlama is a great way to experiment with AI on your own machine or on **Google Colab**, there are a few important limitations to be aware of:\n",
    "\n",
    "- **Context Window is Small:**  \n",
    "  TinyLlama (and many local models) can only \"read\" a limited number of tokens at once ‚Äî typically **512 to 2048 tokens**. This restricts how much background context you can pass in, which can limit the quality of the model‚Äôs response for more complex queries.\n",
    "\n",
    "- **Performance Depends on Hardware:**  \n",
    "  Even with access to **free GPUs on Google Colab**, your model‚Äôs inference speed and memory capacity are still tied to the available hardware. If you try to push too many tokens or use a larger model, you might encounter **slow responses or out-of-memory errors**.\n",
    "\n",
    "---\n",
    "\n",
    "## Moving to Cloud-Based LLMs\n",
    "\n",
    "To overcome these constraints, the next natural step is to use **cloud-hosted LLMs via API requests** ‚Äî such as those from **OpenAI, Mistral, or Anthropic (Claude)**. These models typically:\n",
    "\n",
    "- Support much **larger context windows** (e.g., 8K, 32K, or even 1M tokens)\n",
    "- Offer **faster and more accurate** responses\n",
    "- Scale well for more advanced, production-grade applications\n",
    "\n",
    "We'll explore how to integrate these cloud-based models later in the course so you can **compare trade-offs** and choose the right tool for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
